# 06_model_interpretability

## Objective

Interpret trained text classification models to understand **which biomedical terms and latent semantic features drive predictions**, assess class separability, and analyze systematic error patterns.

---

## 1. Environment Setup

```python
import numpy as np
import pandas as pd
import joblib
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix

plt.rcParams['figure.figsize'] = (8, 6)
```

---

## 2. Load Models, Encoders, and Features

```python
# Load artifacts
log_reg = joblib.load("../models/logistic_regression.joblib")nsvm = joblib.load("../models/linear_svm.joblib")nlabel_encoder = joblib.load("../models/label_encoder.joblib")ntfidf = joblib.load("../models/tfidf_word_vectorizer.joblib")nsvd = joblib.load("../models/svd_model.joblib")n
# Load data
df = pd.read_parquet("../data/processed/preprocessed_text.parquet")nX_test = np.load("../data/processed/X_test_svd.npy")ny_test = np.load("../data/processed/y_test.npy")n
labels = label_encoder.classes_
```

---

## 3. Confusion Matrix (Best Linear Model)

```python
y_pred = log_reg.predict(X_test)

cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
            xticklabels=labels, yticklabels=labels)
plt.title("Logistic Regression Confusion Matrix")nplt.xlabel("Predicted")nplt.ylabel("True")nplt.show()
```

### Insight

* Highlights systematic confusion between biologically adjacent cancer types
* Guides deeper semantic inspection

---

## 4. Mapping Latent SVD Components Back to Words

### 4.1 Extract Component Loadings

```python
components = svd.components_  # shape: [n_components, vocab_size]
terms = tfidf.get_feature_names_out()
```

---

### 4.2 Top Contributing Terms per Latent Dimension

```python
def top_terms_per_component(components, terms, n=10):
    output = {}
    for i, comp in enumerate(components[:10]):
        idx = np.argsort(comp)[-n:]
        output[f"Component_{i}"] = terms[idx]
    return output

latent_terms = top_terms_per_component(components, terms)
latent_terms
```

### Insight

* Latent topics correspond to treatment, diagnosis, genetics, and outcomes
* Confirms semantic validity of dimensionality reduction

---

## 5. Class-Specific Feature Importance (Logistic Regression)

```python
coef = log_reg.coef_  # shape: [n_classes, n_components]
```

```python
def top_latent_features(coef, class_idx, n=10):
    idx = np.argsort(coef[class_idx])[-n:]
    return idx
```

```python
for i, label in enumerate(labels):
    print(f"\nTop latent components for class: {label}")
    print(top_latent_features(coef, i))
```

---

## 6. Error Analysis: Misclassified Documents

```python
df_test = df.iloc[:len(y_test)].copy()
df_test['true_label'] = label_encoder.inverse_transform(y_test)
df_test['pred_label'] = label_encoder.inverse_transform(y_pred)

errors = df_test[df_test['true_label'] != df_test['pred_label']]
errors[['true_label', 'pred_label', 'clean_text']].head(3)
```

### Insight

* Errors often involve shared treatment protocols or genetic markers
* Indicates real biomedical overlap rather than model failure

---

## 7. Model Interpretability Summary

* Linear models enable transparent decision boundaries
* SVD components correspond to meaningful biomedical concepts
* Misclassifications reflect domain-level semantic overlap
* Interpretability confirms model decisions are biologically plausible

➡️ Proceed next to **07_biomedical_insights.ipynb**
