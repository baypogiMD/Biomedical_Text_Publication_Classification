# 04_feature_engineering

## Objective

Transform preprocessed biomedical text into machine‑learning‑ready numerical representations using statistically grounded feature engineering techniques suitable for long scientific documents.

---

## 1. Environment Setup

```python
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.preprocessing import LabelEncoder
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
import joblib

pd.set_option('display.max_colwidth', 120)
```

---

## 2. Load Preprocessed Dataset

```python
df = pd.read_parquet("../data/processed/preprocessed_text.parquet")
print(df.shape)
df[['label', 'clean_text']].head()
```

---

## 3. Encode Target Labels

```python
label_encoder = LabelEncoder()
df['label_encoded'] = label_encoder.fit_transform(df['label'])

label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
label_mapping
```

```python
joblib.dump(label_encoder, "../models/label_encoder.joblib")
```

---

## 4. Train / Test Split

Stratified splitting is critical due to possible class imbalance.

```python
X_train, X_test, y_train, y_test = train_test_split(
    df['clean_text'],
    df['label_encoded'],
    test_size=0.2,
    random_state=42,
    stratify=df['label_encoded']
)
```

---

## 5. TF‑IDF Feature Engineering (Primary Representation)

### 5.1 Word‑Level TF‑IDF

```python
tfidf_word = TfidfVectorizer(
    max_features=30000,
    ngram_range=(1, 2),
    min_df=5,
    max_df=0.9,
    sublinear_tf=True
)
```

```python
X_train_word = tfidf_word.fit_transform(X_train)
X_test_word = tfidf_word.transform(X_test)

X_train_word.shape, X_test_word.shape
```

---

### 5.2 Character‑Level TF‑IDF (Biomedical Robustness)

Captures morphology of gene/protein names and rare terminology.

```python
tfidf_char = TfidfVectorizer(
    analyzer='char',
    ngram_range=(3, 5),
    min_df=5,
    max_features=20000
)
```

```python
X_train_char = tfidf_char.fit_transform(X_train)
X_test_char = tfidf_char.transform(X_test)

X_train_char.shape
```

---

## 6. Dimensionality Reduction with Truncated SVD (LSA)

Reduces sparsity and improves downstream model stability.

```python
svd = TruncatedSVD(n_components=300, random_state=42)

X_train_svd = svd.fit_transform(X_train_word)
X_test_svd = svd.transform(X_test_word)

X_train_svd.shape
```

```python
explained_variance = svd.explained_variance_ratio_.sum()
print(f"Explained variance: {explained_variance:.2%}")
```

---

## 7. Persist Engineered Features

```python
joblib.dump(tfidf_word, "../models/tfidf_word_vectorizer.joblib")
joblib.dump(tfidf_char, "../models/tfidf_char_vectorizer.joblib")
joblib.dump(svd, "../models/svd_model.joblib")
```

```python
np.save("../data/processed/X_train_svd.npy", X_train_svd)
np.save("../data/processed/X_test_svd.npy", X_test_svd)
np.save("../data/processed/y_train.npy", y_train)
np.save("../data/processed/y_test.npy", y_test)
```

---

## 8. Feature Sanity Checks

```python
assert X_train_svd.shape[0] == y_train.shape[0]
assert X_test_svd.shape[0] == y_test.shape[0]
```

---

## 9. Feature Engineering Summary

* Word‑level TF‑IDF captures semantic content
* Character‑level TF‑IDF increases robustness to biomedical tokens
* Truncated SVD reduces dimensionality while retaining signal
* Features serialized for reproducible modeling

➡️ Proceed next to **05_predictive_modeling.ipynb**
