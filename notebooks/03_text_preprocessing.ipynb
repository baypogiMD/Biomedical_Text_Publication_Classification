# 03_text_preprocessing

## Objective

Apply robust biomedical-aware text preprocessing to transform raw publication text into clean, normalized representations suitable for feature engineering and modeling, while preserving clinically meaningful terminology.

---

## 1. Environment Setup

```python
import pandas as pd
import numpy as np
import re
import spacy
from tqdm import tqdm

# Load spaCy English model
# python -m spacy download en_core_web_sm
nlp = spacy.load("en_core_web_sm", disable=["ner", "parser"])

pd.set_option('display.max_colwidth', 120)
tqdm.pandas()
```

---

## 2. Load Clean Dataset

```python
df = pd.read_parquet("../data/processed/cleaned_text.parquet")
print(df.shape)
df.head()
```

---

## 3. Define Biomedical-Safe Preprocessing Rules

### 3.1 Custom Stopwords

We extend default English stopwords with publication artifacts while **preserving biomedical tokens** (e.g., gene names, abbreviations).

```python
custom_stopwords = set([
    'et', 'al', 'figure', 'fig', 'table', 'supplementary',
    'introduction', 'methods', 'results', 'discussion', 'conclusion'
])

stopwords = nlp.Defaults.stop_words.union(custom_stopwords)
```

---

### 3.2 Regex Cleaning Rules

```python
CITATION_PATTERN = r"\\(.*?\\d{4}.*?\\)"  # (Smith et al., 2020)
REFERENCE_PATTERN = r"\\[\\d+\\]"       # [12]
NON_ALPHA_PATTERN = r"[^a-zA-Z0-9\\s-]"
MULTISPACE_PATTERN = r"\\s+"
```

```python
def basic_clean(text):
    text = re.sub(CITATION_PATTERN, ' ', text)
    text = re.sub(REFERENCE_PATTERN, ' ', text)
    text = re.sub(NON_ALPHA_PATTERN, ' ', text)
    text = re.sub(MULTISPACE_PATTERN, ' ', text)
    return text.strip().lower()
```

---

## 4. Lemmatization with Term Preservation

### Rules:

* Keep alphanumeric biomedical terms (e.g., BRCA1, p53)
* Lemmatize common English words
* Drop stopwords and very short tokens

```python
def lemmatize_text(text):
    doc = nlp(text)
    tokens = []

    for token in doc:
        if token.text in stopwords:
            continue
        if len(token.text) < 3:
            continue
        if token.text.isalnum() and any(char.isdigit() for char in token.text):
            tokens.append(token.text.lower())  # Preserve biomedical IDs
        else:
            tokens.append(token.lemma_.lower())

    return " ".join(tokens)
```

---

## 5. Apply Preprocessing Pipeline

```python
df['clean_text'] = df['full_text'].progress_apply(basic_clean)
df['clean_text'] = df['clean_text'].progress_apply(lemmatize_text)
```

---

## 6. Quality Checks

```python
df[['full_text', 'clean_text']].sample(5)
```

```python
# Token count comparison
df['clean_word_count'] = df['clean_text'].str.split().str.len()

df[['word_count', 'clean_word_count']].describe()
```

---

## 7. Drop Degenerate Samples

```python
before = len(df)
df = df[df['clean_word_count'] >= 50].reset_index(drop=True)
after = len(df)

print(f"Removed {before - after} low-information documents")
```

---

## 8. Persist Preprocessed Dataset

```python
output_path = "../data/processed/preprocessed_text.parquet"
df.to_parquet(output_path, index=False)
```

```python
csv_path = "../data/processed/preprocessed_text.csv"
df.to_csv(csv_path, index=False)
```

---

## 9. Summary

* Citation noise and formatting artifacts removed
* Biomedical terms preserved
* Lemmatization applied selectively
* Stopwords tailored to scientific writing
* Dataset prepared for feature engineering and modeling

➡️ Proceed next to **04_feature_engineering.ipynb**
