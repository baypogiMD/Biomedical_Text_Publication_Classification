# 02_exploratory_text_analysis

## Objective

Perform exploratory analysis on biomedical publication text to understand label distribution, document length characteristics, vocabulary properties, and high-level linguistic patterns prior to preprocessing and modeling.

---

## 1. Environment Setup

```python
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from collections import Counter
from sklearn.feature_extraction.text import CountVectorizer

plt.rcParams['figure.figsize'] = (10, 5)
sns.set_style("darkgrid")
```

---

## 2. Load Clean Dataset

```python
df = pd.read_parquet("../data/processed/cleaned_text.parquet")
print(df.shape)
df.head()
```

---

## 3. Label Distribution Analysis

```python
label_counts = df['label'].value_counts()
label_counts
```

```python
sns.barplot(x=label_counts.index, y=label_counts.values)
plt.title("Label Distribution")
plt.ylabel("Number of Documents")
plt.xlabel("Cancer Category")
plt.xticks(rotation=45)
plt.show()
```

### Insight

* Identify class imbalance
* Inform stratified sampling and evaluation strategy

---

## 4. Document Length Analysis

```python
sns.histplot(df['word_count'], bins=50, kde=True)
plt.title("Distribution of Document Word Counts")
plt.xlabel("Word Count")
plt.show()
```

```python
df.groupby('label')['word_count'].describe()
```

### Insight

* Biomedical publications are long-form texts
* Different cancer categories may have systematically different lengths

---

## 5. Vocabulary Size and Lexical Richness

```python
vectorizer = CountVectorizer(stop_words='english', max_features=20000)
X = vectorizer.fit_transform(df['full_text'])

vocab_size = len(vectorizer.vocabulary_)
vocab_size
```

```python
avg_words = df['word_count'].mean()
unique_words = vocab_size

lexical_ratio = unique_words / avg_words
lexical_ratio
```

---

## 6. Most Frequent Unigrams (Corpus-Level)

```python
word_counts = np.asarray(X.sum(axis=0)).ravel()
vocab = vectorizer.get_feature_names_out()

word_freq = pd.DataFrame({'word': vocab, 'count': word_counts})
word_freq = word_freq.sort_values(by='count', ascending=False)

word_freq.head(20)
```

```python
sns.barplot(x='count', y='word', data=word_freq.head(20))
plt.title("Top 20 Most Frequent Words (Corpus)")
plt.show()
```

### Insight

* Reveals dominant biomedical terminology
* Highlights need for domain-specific stopword refinement

---

## 7. Class-Specific Vocabulary Patterns

```python
class_vocab = {}

for label in df['label'].unique():
    text = df[df['label'] == label]['full_text']
    vec = CountVectorizer(stop_words='english', max_features=5000)
    X_label = vec.fit_transform(text)
    counts = np.asarray(X_label.sum(axis=0)).ravel()
    words = vec.get_feature_names_out()
    class_vocab[label] = pd.DataFrame({'word': words, 'count': counts}) \
                          .sort_values(by='count', ascending=False).head(15)
```

```python
for label, vocab_df in class_vocab.items():
    print(f"\nTop words for {label}:")
    display(vocab_df)
```

### Insight

* Identifies discriminative biomedical terms per cancer type
* Guides feature engineering and interpretability

---

## 8. Bigram Analysis

```python
bigram_vectorizer = CountVectorizer(
    stop_words='english',
    ngram_range=(2, 2),
    max_features=20
)

X_bi = bigram_vectorizer.fit_transform(df['full_text'])

bigrams = pd.DataFrame({
    'bigram': bigram_vectorizer.get_feature_names_out(),
    'count': np.asarray(X_bi.sum(axis=0)).ravel()
}).sort_values(by='count', ascending=False)

bigrams
```

---

## 9. Zipf's Law Validation (Optional)

```python
word_freq_sorted = word_freq.sort_values(by='count', ascending=False)
word_freq_sorted['rank'] = range(1, len(word_freq_sorted) + 1)

plt.loglog(word_freq_sorted['rank'], word_freq_sorted['count'])
plt.title("Zipf's Law in Biomedical Text")
plt.xlabel("Rank (log)")
plt.ylabel("Frequency (log)")
plt.show()
```

---

## 10. Key Findings Summary

* Dataset shows moderate class imbalance
* Publications are long and information-dense
* Strong domain-specific vocabulary dominance
* Clear class-specific lexical signals
* N-gram patterns support separability between cancer categories

➡️ Proceed next to **03_text_preprocessing.ipynb**
