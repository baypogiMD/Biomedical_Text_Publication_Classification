# 01_data_loading_and_validation

## Objective

Load the biomedical text publication dataset, validate its structure, perform initial quality checks, and persist a clean version for downstream SQL and NLP analysis.

---

## 1. Environment Setup

```python
import pandas as pd
import numpy as np
import os
from pathlib import Path
```

---

## 2. File Paths and Configuration

```python
DATA_DIR = Path("../data")
RAW_DIR = DATA_DIR / "raw"
PROCESSED_DIR = DATA_DIR / "processed"

RAW_DIR.mkdir(parents=True, exist_ok=True)
PROCESSED_DIR.mkdir(parents=True, exist_ok=True)

DATA_FILE = RAW_DIR / "biomedical_text.csv"
```

---

## 3. Load Dataset

```python
df = pd.read_csv(DATA_FILE)
print("Dataset shape:", df.shape)
df.head()
```

---

## 4. Schema Validation

```python
print(df.info())
```

Expected columns (may vary slightly by Kaggle version):

* `title`
* `text` or `full_text`
* `label`

```python
required_columns = {"text", "label"}
missing_cols = required_columns - set(df.columns)

if missing_cols:
    raise ValueError(f"Missing required columns: {missing_cols}")
```

---

## 5. Basic Data Quality Checks

### Missing Values

```python
df.isnull().sum()
```

```python
df = df.dropna(subset=["text", "label"]).reset_index(drop=True)
```

---

### Duplicate Documents

```python
duplicates = df.duplicated(subset=["text"]).sum()
print("Duplicate documents:", duplicates)

if duplicates > 0:
    df = df.drop_duplicates(subset=["text"]).reset_index(drop=True)
```

---

## 6. Label Validation

```python
label_counts = df["label"].value_counts()
label_counts
```

```python
print("Number of classes:", df["label"].nunique())
```

---

## 7. Text Statistics

```python
df["word_count"] = df["text"].str.split().str.len()

df[["word_count"]].describe()
```

---

## 8. Sanity Checks on Text Length

```python
short_docs = df[df["word_count"] < 50]
print("Documents with <50 words:", len(short_docs))
```

```python
# Optional: remove extremely short documents
df = df[df["word_count"] >= 50].reset_index(drop=True)
```

---

## 9. Standardize Column Names

```python
df = df.rename(columns={
    "text": "full_text"
})
```

---

## 10. Final Dataset Overview

```python
print(df.shape)
df.head()
```

---

## 11. Persist Clean Dataset

### Save as Parquet (for notebooks)

```python
clean_path = PROCESSED_DIR / "cleaned_text.parquet"
df.to_parquet(clean_path, index=False)
```

### Save as CSV (for SQL ingestion)

```python
sql_path = PROCESSED_DIR / "cleaned_text.csv"
df.to_csv(sql_path, index=False)
```

---

## 12. Summary

* Dataset loaded and validated
* Missing and duplicate entries handled
* Labels verified
* Text length sanity checks applied
* Clean dataset persisted for SQL and modeling stages

➡️ Proceed next to **02_exploratory_text_analysis.ipynb**
